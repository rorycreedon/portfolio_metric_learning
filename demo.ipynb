{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yfinance as yf\n",
    "\n",
    "# Other files\n",
    "import utils\n",
    "from models.autowarp import AutoWarp\n",
    "from models.mean_variance_optimisation import MeanVarianceOptimisation\n",
    "from models.autoencoders import LinearAutoencoder, ConvAutoencoder, ConvLinearAutoEncoder, train_autoencoder\n",
    "from models.fama_french import FamaFrench\n",
    "\n",
    "# General imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "# Optimisation\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.hierarchical_portfolio import HRPOpt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Plotting adjustments\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams.update({'figure.autolayout': True})"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 years training (split into 2 year train and 1 year valid), 18 months test\n",
    "start_date = '2018-09-01'\n",
    "valid_date = '2020-09-01'\n",
    "train_date = '2021-09-01'\n",
    "end_date   = '2023-03-01'\n",
    "\n",
    "# Open json file with parameters\n",
    "with open(f'params/sp500_{start_date}.json') as f:\n",
    "    params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Download data\n",
    "# data_dict = {'Linear': {}, 'CNN': {}, 'Linear + CNN': {}}\n",
    "# for model in ['Linear', 'CNN', 'Linear + CNN']:\n",
    "#     data_arrays, price_dfs = utils.split_data(start_date, valid_date, train_date, end_date, train_valid_split=2 / 3, **params[model]['sharpe']['data'])\n",
    "#     data_dict[model]['data_train'] = data_arrays[0]\n",
    "#     assert np.isnan(data_arrays[0]).sum() == 0\n",
    "\n",
    "data_arrays, price_dfs = utils.split_data(start_date=start_date, valid_date=valid_date, train_date=train_date, end_date=end_date, train_valid_split=2 / 3, returns=False, momentum=False)\n",
    "\n",
    "prices_train = price_dfs[0]\n",
    "prices_test = price_dfs[3]\n",
    "data_train = data_arrays[0]\n",
    "\n",
    "num_epochs=20"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# S&P 500\n",
    "sp500 = yf.download(\"^GSPC\", start=train_date, end=end_date, period=\"1d\", progress=False)['Adj Close']\n",
    "sp500 = sp500.div(sp500.iloc[0]).mul(100)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating autoencoder distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearAutoencoder\n",
      "Epoch [1/20], Loss: 0.0635\n",
      "Epoch [2/20], Loss: 0.0132\n",
      "Epoch [3/20], Loss: 0.0093\n",
      "Epoch [4/20], Loss: 0.0079\n",
      "Epoch [5/20], Loss: 0.0072\n",
      "Epoch [6/20], Loss: 0.0068\n",
      "Epoch [7/20], Loss: 0.0063\n",
      "Epoch [8/20], Loss: 0.0056\n",
      "Epoch [9/20], Loss: 0.0052\n",
      "Epoch [10/20], Loss: 0.0048\n",
      "Epoch [11/20], Loss: 0.0044\n",
      "Epoch [12/20], Loss: 0.0042\n",
      "Epoch [13/20], Loss: 0.0041\n",
      "Epoch [14/20], Loss: 0.0039\n",
      "Epoch [15/20], Loss: 0.0037\n",
      "Epoch [16/20], Loss: 0.0035\n",
      "Epoch [17/20], Loss: 0.0034\n",
      "Epoch [18/20], Loss: 0.0032\n",
      "Epoch [19/20], Loss: 0.0031\n",
      "Epoch [20/20], Loss: 0.0030\n",
      "ConvAutoencoder\n",
      "Epoch [1/20], Loss: 0.1258\n",
      "Epoch [2/20], Loss: 0.0665\n",
      "Epoch [3/20], Loss: 0.0349\n",
      "Epoch [4/20], Loss: 0.0229\n",
      "Epoch [5/20], Loss: 0.0190\n",
      "Epoch [6/20], Loss: 0.0168\n",
      "Epoch [7/20], Loss: 0.0150\n",
      "Epoch [8/20], Loss: 0.0138\n",
      "Epoch [9/20], Loss: 0.0130\n",
      "Epoch [10/20], Loss: 0.0124\n",
      "Epoch [11/20], Loss: 0.0119\n",
      "Epoch [12/20], Loss: 0.0115\n",
      "Epoch [13/20], Loss: 0.0111\n",
      "Epoch [14/20], Loss: 0.0107\n",
      "Epoch [15/20], Loss: 0.0105\n",
      "Epoch [16/20], Loss: 0.0103\n",
      "Epoch [17/20], Loss: 0.0102\n",
      "Epoch [18/20], Loss: 0.0101\n",
      "Epoch [19/20], Loss: 0.0100\n",
      "Epoch [20/20], Loss: 0.0100\n",
      "ConvLinearAutoEncoder\n",
      "Epoch [1/20], Loss: 0.1339\n",
      "Epoch [2/20], Loss: 0.0692\n",
      "Epoch [3/20], Loss: 0.0568\n",
      "Epoch [4/20], Loss: 0.0535\n",
      "Epoch [5/20], Loss: 0.0498\n",
      "Epoch [6/20], Loss: 0.0467\n",
      "Epoch [7/20], Loss: 0.0429\n",
      "Epoch [8/20], Loss: 0.0378\n",
      "Epoch [9/20], Loss: 0.0327\n",
      "Epoch [10/20], Loss: 0.0305\n",
      "Epoch [11/20], Loss: 0.0304\n",
      "Epoch [12/20], Loss: 0.0302\n",
      "Epoch [13/20], Loss: 0.0299\n",
      "Epoch [14/20], Loss: 0.0296\n",
      "Epoch [15/20], Loss: 0.0296\n",
      "Epoch [16/20], Loss: 0.0296\n",
      "Epoch [17/20], Loss: 0.0295\n",
      "Epoch [18/20], Loss: 0.0295\n",
      "Epoch [19/20], Loss: 0.0295\n",
      "Epoch [20/20], Loss: 0.0294\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "models['Linear'] = train_autoencoder(LinearAutoencoder, input_size=data_train.shape[1], num_epochs=num_epochs, data=data_train, verbose=True, **params['Linear']['sharpe']['autoencoder'])\n",
    "models['CNN'] = train_autoencoder(ConvAutoencoder, input_size=data_train.shape[1], num_epochs=num_epochs, data=data_train, verbose=True, **params['CNN']['sharpe']['autoencoder'])\n",
    "models['Linear + CNN'] =train_autoencoder(ConvLinearAutoEncoder, input_size=data_train.shape[1], num_epochs=num_epochs, data=data_train, verbose=True, **params['Linear + CNN']['sharpe']['autoencoder'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear\n",
      "Alpha: 0.4150773882865906\n",
      "Gamma: 0.22196239233016968\n",
      "Epsilon: 0.7845350503921509\n",
      "Linear + CNN\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'ValueError' and 'ValueError'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[1;32m<timed exec>:5\u001B[0m\n",
      "File \u001B[1;32m~\\Documents\\_Git\\portfolio_metric_learning\\models\\autowarp.py:242\u001B[0m, in \u001B[0;36mAutoWarp.learn_metric\u001B[1;34m(self, verbose)\u001B[0m\n\u001B[0;32m    237\u001B[0m euclidian_distance \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39meuclidian_distance()\n\u001B[0;32m    239\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m convergence \u001B[38;5;129;01mand\u001B[39;00m iteration \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_iterations:\n\u001B[0;32m    240\u001B[0m \n\u001B[0;32m    241\u001B[0m     \u001B[38;5;66;03m# Sample S pairs of trajectories from the euclidian distance matrix\u001B[39;00m\n\u001B[1;32m--> 242\u001B[0m     close_pairs, all_pairs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msample_trajectory_pairs\u001B[49m\u001B[43m(\u001B[49m\u001B[43meuclidian_distance\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    244\u001B[0m     \u001B[38;5;66;03m# Compute gradients and beta_hat\u001B[39;00m\n\u001B[0;32m    245\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m iteration \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[1;32m~\\Documents\\_Git\\portfolio_metric_learning\\models\\autowarp.py:67\u001B[0m, in \u001B[0;36mAutoWarp.sample_trajectory_pairs\u001B[1;34m(self, euclidian_distance)\u001B[0m\n\u001B[0;32m     60\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     61\u001B[0m \u001B[38;5;124;03mSamples pairs of trajectories from the latent space\u001B[39;00m\n\u001B[0;32m     62\u001B[0m \u001B[38;5;124;03m:param euclidian_distance: Euclidean distance between the latent representation of each stock\u001B[39;00m\n\u001B[0;32m     63\u001B[0m \u001B[38;5;124;03m:return: close_pairs, all_pairs\u001B[39;00m\n\u001B[0;32m     64\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;66;03m# Compute delta\u001B[39;00m\n\u001B[1;32m---> 67\u001B[0m delta \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpercentile\u001B[49m\u001B[43m(\u001B[49m\u001B[43meuclidian_distance\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mp\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;66;03m# Create a mask for pairs with distance in the latent space less than delta\u001B[39;00m\n\u001B[0;32m     70\u001B[0m close_pairs_mask \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mtriu(euclidian_distance \u001B[38;5;241m<\u001B[39m delta, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[1;32m<__array_function__ internals>:180\u001B[0m, in \u001B[0;36mpercentile\u001B[1;34m(*args, **kwargs)\u001B[0m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\metric_learning_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4166\u001B[0m, in \u001B[0;36mpercentile\u001B[1;34m(a, q, axis, out, overwrite_input, method, keepdims, interpolation)\u001B[0m\n\u001B[0;32m   4164\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _quantile_is_valid(q):\n\u001B[0;32m   4165\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPercentiles must be in the range [0, 100]\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 4166\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_quantile_unchecked\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   4167\u001B[0m \u001B[43m    \u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moverwrite_input\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkeepdims\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\metric_learning_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4424\u001B[0m, in \u001B[0;36m_quantile_unchecked\u001B[1;34m(a, q, axis, out, overwrite_input, method, keepdims)\u001B[0m\n\u001B[0;32m   4416\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_quantile_unchecked\u001B[39m(a,\n\u001B[0;32m   4417\u001B[0m                         q,\n\u001B[0;32m   4418\u001B[0m                         axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4421\u001B[0m                         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlinear\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   4422\u001B[0m                         keepdims\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[0;32m   4423\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Assumes that q is in [0, 1], and is an ndarray\"\"\"\u001B[39;00m\n\u001B[1;32m-> 4424\u001B[0m     r, k \u001B[38;5;241m=\u001B[39m \u001B[43m_ureduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43ma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4425\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mfunc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_quantile_ureduce_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4426\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mq\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4427\u001B[0m \u001B[43m                    \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4428\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4429\u001B[0m \u001B[43m                    \u001B[49m\u001B[43moverwrite_input\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moverwrite_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4430\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4431\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m keepdims:\n\u001B[0;32m   4432\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m r\u001B[38;5;241m.\u001B[39mreshape(q\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m+\u001B[39m k)\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\metric_learning_env\\lib\\site-packages\\numpy\\lib\\function_base.py:3725\u001B[0m, in \u001B[0;36m_ureduce\u001B[1;34m(a, func, **kwargs)\u001B[0m\n\u001B[0;32m   3722\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3723\u001B[0m     keepdim \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m1\u001B[39m,) \u001B[38;5;241m*\u001B[39m a\u001B[38;5;241m.\u001B[39mndim\n\u001B[1;32m-> 3725\u001B[0m r \u001B[38;5;241m=\u001B[39m func(a, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   3726\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m r, keepdim\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\metric_learning_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4593\u001B[0m, in \u001B[0;36m_quantile_ureduce_func\u001B[1;34m(a, q, axis, out, overwrite_input, method)\u001B[0m\n\u001B[0;32m   4591\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   4592\u001B[0m         arr \u001B[38;5;241m=\u001B[39m a\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m-> 4593\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43m_quantile\u001B[49m\u001B[43m(\u001B[49m\u001B[43marr\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4594\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mquantiles\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mq\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4595\u001B[0m \u001B[43m                   \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4596\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mmethod\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmethod\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4597\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4598\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\metric_learning_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4710\u001B[0m, in \u001B[0;36m_quantile\u001B[1;34m(arr, quantiles, axis, method, out)\u001B[0m\n\u001B[0;32m   4708\u001B[0m     result_shape \u001B[38;5;241m=\u001B[39m virtual_indexes\u001B[38;5;241m.\u001B[39mshape \u001B[38;5;241m+\u001B[39m (\u001B[38;5;241m1\u001B[39m,) \u001B[38;5;241m*\u001B[39m (arr\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m   4709\u001B[0m     gamma \u001B[38;5;241m=\u001B[39m gamma\u001B[38;5;241m.\u001B[39mreshape(result_shape)\n\u001B[1;32m-> 4710\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[43m_lerp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprevious\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4711\u001B[0m \u001B[43m                   \u001B[49m\u001B[38;5;28;43mnext\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4712\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   4713\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mout\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4714\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39many(slices_having_nans):\n\u001B[0;32m   4715\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m result\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m out \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   4716\u001B[0m         \u001B[38;5;66;03m# can't write to a scalar\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\envs\\metric_learning_env\\lib\\site-packages\\numpy\\lib\\function_base.py:4527\u001B[0m, in \u001B[0;36m_lerp\u001B[1;34m(a, b, t, out)\u001B[0m\n\u001B[0;32m   4513\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_lerp\u001B[39m(a, b, t, out\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m   4514\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m   4515\u001B[0m \u001B[38;5;124;03m    Compute the linear interpolation weighted by gamma on each point of\u001B[39;00m\n\u001B[0;32m   4516\u001B[0m \u001B[38;5;124;03m    two same shape array.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   4525\u001B[0m \u001B[38;5;124;03m        Output array.\u001B[39;00m\n\u001B[0;32m   4526\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m-> 4527\u001B[0m     diff_b_a \u001B[38;5;241m=\u001B[39m \u001B[43msubtract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43ma\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   4528\u001B[0m     \u001B[38;5;66;03m# asanyarray is a stop-gap until gh-13105\u001B[39;00m\n\u001B[0;32m   4529\u001B[0m     lerp_interpolation \u001B[38;5;241m=\u001B[39m asanyarray(add(a, diff_b_a \u001B[38;5;241m*\u001B[39m t, out\u001B[38;5;241m=\u001B[39mout))\n",
      "\u001B[1;31mTypeError\u001B[0m: unsupported operand type(s) for -: 'ValueError' and 'ValueError'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dist_matrices = {}\n",
    "for model in ['Linear', 'Linear + CNN', 'CNN']:\n",
    "    print(model)\n",
    "    learner = AutoWarp(models[model], data_train, **params[model]['sharpe']['dist_matrix'])\n",
    "    learner.learn_metric(verbose=True)\n",
    "    dist_matrices[model] = learner.create_distance_matrix()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Mean variance optimisation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear train SR: 1.7469835067284967\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'CNN'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 11\u001B[0m\n\u001B[0;32m      8\u001B[0m optimiser \u001B[38;5;241m=\u001B[39m MeanVarianceOptimisation(expected_returns \u001B[38;5;241m=\u001B[39m e_returns, prices \u001B[38;5;241m=\u001B[39m prices_train, solver\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mECOS\u001B[39m\u001B[38;5;124m'\u001B[39m, weight_bounds \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m1\u001B[39m))\n\u001B[0;32m     10\u001B[0m \u001B[38;5;66;03m# Get weights\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m risk_matrix \u001B[38;5;241m=\u001B[39m optimiser\u001B[38;5;241m.\u001B[39mmake_risk_matrix(\u001B[43mdist_matrices\u001B[49m\u001B[43m[\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m]\u001B[49m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mparams[model][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msharpe\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrisk_matrix\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m     12\u001B[0m weights[model], train_sr \u001B[38;5;241m=\u001B[39m optimiser\u001B[38;5;241m.\u001B[39mmax_sharpe_ratio(risk_matrix\u001B[38;5;241m=\u001B[39mrisk_matrix, l2_reg\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     14\u001B[0m \u001B[38;5;66;03m# Print out summary\u001B[39;00m\n",
      "\u001B[1;31mKeyError\u001B[0m: 'CNN'"
     ]
    }
   ],
   "source": [
    "# Empty dict for weights\n",
    "weights = {}\n",
    "\n",
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\"]:\n",
    "\n",
    "    # Setup\n",
    "    e_returns = mean_historical_return(prices_train)\n",
    "    optimiser = MeanVarianceOptimisation(expected_returns = e_returns, prices = prices_train, solver='ECOS', weight_bounds = (0,1))\n",
    "\n",
    "    # Get weights\n",
    "    risk_matrix = optimiser.make_risk_matrix(dist_matrices[model], **params[model]['sharpe']['risk_matrix'])\n",
    "    weights[model], train_sr = optimiser.max_sharpe_ratio(risk_matrix=risk_matrix, l2_reg=0)\n",
    "\n",
    "    # Print out summary\n",
    "    print(model, \"train SR:\", train_sr)\n",
    "\n",
    "for model in [\"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\"]:\n",
    "\n",
    "    # Setup\n",
    "    e_returns = mean_historical_return(prices_train)\n",
    "    optimiser = MeanVarianceOptimisation(expected_returns = e_returns, prices = prices_train, solver='ECOS', weight_bounds = (0,1))\n",
    "\n",
    "    # Get weights\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        risk_matrix = optimiser.benchmark_matrix(model)\n",
    "        weights[model], train_sr = optimiser.max_sharpe_ratio(risk_matrix=risk_matrix, l2_reg=0)\n",
    "\n",
    "    # Print out summary\n",
    "    print(model, \"train SR:\", train_sr)\n",
    "\n",
    "# Factor model\n",
    "e_returns = mean_historical_return(prices_train)\n",
    "optimiser = MeanVarianceOptimisation(expected_returns = e_returns, prices = prices_train, solver='ECOS', weight_bounds = (0,1))\n",
    "fama_french = FamaFrench(prices_train, file_path='data/F-F_Research_Data_Factors_daily.CSV', n_rows=25419)\n",
    "fama_cov = fama_french.get_covariance_matrix()\n",
    "weights['Fama-French 3 factor'], train_sr = optimiser.max_sharpe_ratio(risk_matrix=fama_cov, l2_reg=0)\n",
    "print(\"Fama-French 3 factor train SR:\", train_sr)\n",
    "\n",
    "# HRP\n",
    "rets = (prices_train/prices_train.shift(1)-1).dropna()\n",
    "hrp = HRPOpt(rets)\n",
    "hrp.optimize()\n",
    "weights['HRP'] = pd.DataFrame.from_dict(hrp.clean_weights(), orient='index', columns=['weights'])\n",
    "train_sr = utils.calculate_sharpe_ratio(prices = prices_test, weights = weights['HRP'])\n",
    "print(\"HRP train SR:\", train_sr)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaulating performance on the test set"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Plots\n",
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\", \"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\", \"Fama-French 3 factor\", \"HRP\"]:\n",
    "    plt.plot(prices_test @ weights[model], label=model)\n",
    "plt.plot(sp500, label=\"S&P 500\")\n",
    "plt.plot(prices_test, alpha=0.05)\n",
    "plt.ylim(50, 200)\n",
    "plt.grid()\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.legend(ncol=2);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Sharpe ratio for each model\n",
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\", \"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\", \"Fama-French 3 factor\", \"HRP\"]:\n",
    "    print(model, \"sharpe\",  utils.calculate_sharpe_ratio(prices = prices_test, weights = weights[model]))\n",
    "print(\"S&P 500:\", utils.calculate_sp500_sharpe(train_date, end_date))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\", \"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\", \"Fama-French 3 factor\", \"HRP\"]:\n",
    "    print(model, \"max drawdown:\", utils.calculate_max_drawdown(prices_test, weights[model])*100)\n",
    "print(\"S&P 500:\", utils.calculate_sp500_drawdown(train_date, end_date))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\", \"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\", \"Fama-French 3 factor\", \"HRP\"]:\n",
    "    print(model, \"number of stocks included:\", np.count_nonzero(weights[model]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plotting all results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results = {}\n",
    "for start_date in [\"2017-03-01\", \"2017-09-01\", \"2018-03-01\", \"2018-09-01\"]:\n",
    "    results[start_date] = {}\n",
    "    for opt in ['sharpe', 'volatility']:\n",
    "   # Open json file with parameters\n",
    "        with open(f'results/sp500_{start_date}_{opt}.json') as f:\n",
    "            results[start_date][opt] = json.load(f)['sharpe_ratios']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# convert dict of dicts to dataframe\n",
    "results_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "                           for i in results.keys()\n",
    "                           for j in results[i].keys()},\n",
    "                       orient='index')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Seperate dataframe for when second index is 'sharpe' and 'volatility'\n",
    "results_sharpe = results_df[results_df.index.get_level_values(1) == 'sharpe'].droplevel(1)\n",
    "results_volatility = results_df[results_df.index.get_level_values(1) == 'volatility'].droplevel(1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colours = ['indianred', 'brown', 'maroon', 'slateblue', 'mediumslateblue', 'darkslateblue', 'mediumseagreen', 'gold', 'gray']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(4, 6))\n",
    "i=0\n",
    "for date in results_sharpe.index:\n",
    "\n",
    "    # Calculate test period\n",
    "    test_start = datetime.strptime(date, '%Y-%m-%d') + relativedelta(years=3)\n",
    "    test_end = test_start + relativedelta(years=1, months=6)\n",
    "\n",
    "    # Plotting\n",
    "    ax.flatten()[i].bar(results_sharpe[results_sharpe.index==date].columns, np.squeeze(results_sharpe[results_sharpe.index==date].values), color = colours)\n",
    "    plt.setp(ax.flatten()[i].get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "    plt.setp(ax.flatten()[i].get_yticklabels(), fontsize=8)\n",
    "    min_y = np.squeeze(results_sharpe[results_sharpe.index==date].values).min()\n",
    "    max_y = np.squeeze(results_sharpe[results_sharpe.index==date].values).max()\n",
    "    if min_y<0 and max_y<0:\n",
    "        max_y-=0.1\n",
    "        min_y -= 0.1\n",
    "    elif min_y>0 and max_y>0:\n",
    "        min_y=0\n",
    "    ax.flatten()[i].set_ylim(top = max_y+0.1, bottom = min_y)\n",
    "    ax.flatten()[i].set_title(f\"{test_start.strftime('%Y-%m-%d')} - {test_end.strftime('%Y-%m-%d')}\", fontsize = 8)\n",
    "\n",
    "    # Add values to bars\n",
    "    for j, v in enumerate(np.squeeze(results_sharpe[results_sharpe.index==date].values)):\n",
    "        if v > 0:\n",
    "            ax.flatten()[i].text(j, v+0.02, \"{:.2f}\".format(v), color='black', fontsize=6, ha='center')\n",
    "        else:\n",
    "            ax.flatten()[i].text(j, v-0.03, \"{:.2f}\".format(v), color='black', fontsize=6, ha='center')\n",
    "\n",
    "    i+=1\n",
    "\n",
    "ax[0,0].set_ylabel('Sharpe ratio', fontsize=9)\n",
    "ax[1,0].set_ylabel('Sharpe ratio', fontsize=9)\n",
    "\n",
    "fig.suptitle('Maximising the Sharpe Ratio', fontsize=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('plots/max_sharpe_results.png')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "colours = ['indianred', 'brown', 'maroon', 'slateblue', 'mediumslateblue', 'darkslateblue', 'mediumseagreen', 'gold', 'gray']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(4, 6))\n",
    "i=0\n",
    "for date in results_volatility.index:\n",
    "\n",
    "    # Calculate test period\n",
    "    test_start = datetime.strptime(date, '%Y-%m-%d') + relativedelta(years=3)\n",
    "    test_end = test_start + relativedelta(years=1, months=6)\n",
    "\n",
    "    # Plotting\n",
    "    ax.flatten()[i].bar(results_volatility[results_volatility.index==date].columns, np.squeeze(results_volatility[results_volatility.index==date].values), color = colours)\n",
    "    plt.setp(ax.flatten()[i].get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "    plt.setp(ax.flatten()[i].get_yticklabels(), fontsize=8)\n",
    "    min_y = np.squeeze(results_volatility[results_volatility.index==date].values).min()\n",
    "    max_y = np.squeeze(results_volatility[results_volatility.index==date].values).max()\n",
    "    if (min_y<0) and (max_y<0):\n",
    "        max_y-=0.1\n",
    "        min_y -= 0.1\n",
    "    elif (min_y>0) and (max_y>0):\n",
    "        min_y=0\n",
    "    else:\n",
    "        min_y-=0.1\n",
    "    ax.flatten()[i].set_ylim(top = max_y+0.1, bottom = min_y)\n",
    "    ax.flatten()[i].set_title(f\"{test_start.strftime('%Y-%m-%d')} - {test_end.strftime('%Y-%m-%d')}\", fontsize = 8)\n",
    "\n",
    "    # Add values to bars\n",
    "    for j, v in enumerate(np.squeeze(results_volatility[results_volatility.index==date].values)):\n",
    "        if v > 0:\n",
    "            ax.flatten()[i].text(j, v+0.02, \"{:.2f}\".format(v), color='black', fontsize=6, ha='center')\n",
    "        else:\n",
    "            ax.flatten()[i].text(j, v-0.03, \"{:.2f}\".format(v), color='black', fontsize=6, ha='center')\n",
    "\n",
    "    i+=1\n",
    "\n",
    "ax[0,0].set_ylabel('Sharpe ratio', fontsize=9)\n",
    "ax[1,0].set_ylabel('Sharpe ratio', fontsize=9)\n",
    "\n",
    "fig.suptitle('Minimising Volatility', fontsize=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('plots/min_vol_results.png');"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metric_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7701ab843c4e014e6cb61c1bc16af98a2d9e6034c64e4833e09d4584e90230f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
