{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Downloading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yfinance as yf\n",
    "\n",
    "# Other files\n",
    "import utils\n",
    "from models.autowarp import AutoWarp\n",
    "from models.mean_variance_optimisation import MeanVarianceOptimisation\n",
    "from models.autoencoders import LinearAutoencoder, ConvAutoencoder, ConvLinearAutoEncoder, train_autoencoder\n",
    "from models.fama_french import FamaFrench\n",
    "\n",
    "# General imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "# Optimisation\n",
    "from pypfopt.expected_returns import mean_historical_return\n",
    "from pypfopt.hierarchical_portfolio import HRPOpt\n",
    "\n",
    "# Set seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plotting adjustments\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "plt.rcParams.update({'figure.autolayout': True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 years training (split into 2 year train and 1 year valid), 18 months test\n",
    "start_date = '2017-09-01'\n",
    "valid_date = '2019-09-01'\n",
    "train_date = '2020-09-01'\n",
    "end_date   = '2022-03-01'\n",
    "\n",
    "# Open json file with parameters\n",
    "with open(f'params/sp500_{start_date}.json') as f:\n",
    "    params = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "data_arrays, price_dfs = utils.split_data(start_date=start_date, valid_date=valid_date, train_date=train_date, end_date=end_date, train_valid_split=2 / 3, returns=False, momentum=False)\n",
    "\n",
    "prices_train = price_dfs[0]\n",
    "prices_test = price_dfs[3]\n",
    "data_train = data_arrays[0]\n",
    "\n",
    "num_epochs=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# S&P 500\n",
    "sp500 = yf.download(\"^GSPC\", start=train_date, end=end_date, period=\"1d\", progress=False)['Adj Close']\n",
    "sp500 = sp500.div(sp500.iloc[0]).mul(100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating autoencoder distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = {}\n",
    "models['Linear'] = train_autoencoder(LinearAutoencoder, input_size=data_train.shape[1], num_epochs=num_epochs, data=data_train, verbose=True, **params['Linear']['sharpe']['autoencoder'])\n",
    "models['CNN'] = train_autoencoder(ConvAutoencoder, input_size=data_train.shape[1], num_epochs=num_epochs, data=data_train, verbose=True, **params['CNN']['sharpe']['autoencoder'])\n",
    "models['Linear + CNN'] =train_autoencoder(ConvLinearAutoEncoder, input_size=data_train.shape[1], num_epochs=num_epochs, data=data_train, verbose=True, **params['Linear + CNN']['sharpe']['autoencoder'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dist_matrices = {}\n",
    "for model in ['Linear', 'Linear + CNN', 'CNN']:\n",
    "    print(model)\n",
    "    learner = AutoWarp(models[model], data_train, **params[model]['sharpe']['dist_matrix'])\n",
    "    learner.learn_metric(verbose=True)\n",
    "    dist_matrices[model] = learner.create_distance_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Mean variance optimisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Empty dict for weights\n",
    "weights = {}\n",
    "\n",
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\"]:\n",
    "\n",
    "    # Setup\n",
    "    e_returns = mean_historical_return(prices_train)\n",
    "    optimiser = MeanVarianceOptimisation(expected_returns = e_returns, prices = prices_train, solver='ECOS', weight_bounds = (0,1))\n",
    "\n",
    "    # Get weights\n",
    "    risk_matrix = optimiser.make_risk_matrix(dist_matrices[model], **params[model]['sharpe']['risk_matrix'])\n",
    "    weights[model], train_sr = optimiser.max_sharpe_ratio(risk_matrix=risk_matrix, l2_reg=0)\n",
    "\n",
    "    # Print out summary\n",
    "    print(model, \"train SR:\", train_sr)\n",
    "\n",
    "for model in [\"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\"]:\n",
    "\n",
    "    # Setup\n",
    "    e_returns = mean_historical_return(prices_train)\n",
    "    optimiser = MeanVarianceOptimisation(expected_returns = e_returns, prices = prices_train, solver='ECOS', weight_bounds = (0,1))\n",
    "\n",
    "    # Get weights\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        risk_matrix = optimiser.benchmark_matrix(model)\n",
    "        weights[model], train_sr = optimiser.max_sharpe_ratio(risk_matrix=risk_matrix, l2_reg=0)\n",
    "\n",
    "    # Print out summary\n",
    "    print(model, \"train SR:\", train_sr)\n",
    "\n",
    "# Factor model\n",
    "e_returns = mean_historical_return(prices_train)\n",
    "optimiser = MeanVarianceOptimisation(expected_returns = e_returns, prices = prices_train, solver='ECOS', weight_bounds = (0,1))\n",
    "fama_french = FamaFrench(prices_train, file_path='data/F-F_Research_Data_Factors_daily.CSV', n_rows=25419)\n",
    "fama_cov = fama_french.get_covariance_matrix()\n",
    "weights['Fama-French 3 factor'], train_sr = optimiser.max_sharpe_ratio(risk_matrix=fama_cov, l2_reg=0)\n",
    "print(\"Fama-French 3 factor train SR:\", train_sr)\n",
    "\n",
    "# HRP\n",
    "rets = (prices_train/prices_train.shift(1)-1).dropna()\n",
    "hrp = HRPOpt(rets)\n",
    "hrp.optimize()\n",
    "weights['HRP'] = pd.DataFrame.from_dict(hrp.clean_weights(), orient='index', columns=['weights'])\n",
    "train_sr = utils.calculate_sharpe_ratio(prices = prices_test, weights = weights['HRP'])\n",
    "print(\"HRP train SR:\", train_sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaulating performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plots\n",
    "plt.figure(figsize=(6,3))\n",
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\", \"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\", \"Fama-French 3 factor\", \"HRP\"]:\n",
    "    plt.plot(prices_test @ weights[model], label=model)\n",
    "plt.plot(sp500, label=\"S&P 500\")\n",
    "plt.plot(prices_test, alpha=0.05)\n",
    "plt.ylim(75, 150)\n",
    "plt.ylabel(\"Portfolio value (indexed at 100)\", fontsize=10)\n",
    "plt.grid()\n",
    "plt.margins(x=0)\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.tick_params(axis='both', which='major', labelsize=10)\n",
    "plt.title(\"Max Sharpe Ratio Portfolios over period 2020-09-01 to 2022-03-01\", fontsize=11)\n",
    "plt.legend(ncol=2, fontsize='x-small');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sharpe ratio for each model\n",
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\", \"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\", \"Fama-French 3 factor\", \"HRP\"]:\n",
    "    print(model, \"sharpe\",  utils.calculate_sharpe_ratio(prices = prices_test, weights = weights[model]))\n",
    "print(\"S&P 500:\", utils.calculate_sp500_sharpe(train_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\", \"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\", \"Fama-French 3 factor\", \"HRP\"]:\n",
    "    print(model, \"max drawdown:\", utils.calculate_max_drawdown(prices_test, weights[model])*100)\n",
    "print(\"S&P 500:\", utils.calculate_sp500_drawdown(train_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for model in [\"Linear\", \"CNN\", \"Linear + CNN\", \"Covariance\", \"Covariance Shrinkage\", \"EW Covariance\", \"Fama-French 3 factor\", \"HRP\"]:\n",
    "    print(model, \"number of stocks included:\", np.count_nonzero(weights[model]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Plotting all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for start_date in [\"2017-03-01\", \"2017-09-01\", \"2018-03-01\", \"2018-09-01\"]:\n",
    "    results[start_date] = {}\n",
    "    for opt in ['sharpe', 'volatility']:\n",
    "   # Open json file with parameters\n",
    "        with open(f'results/sp500_{start_date}_{opt}.json') as f:\n",
    "            results[start_date][opt] = json.load(f)['sharpe_ratios']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert dict of dicts to dataframe\n",
    "results_df = pd.DataFrame.from_dict({(i,j): results[i][j]\n",
    "                           for i in results.keys()\n",
    "                           for j in results[i].keys()},\n",
    "                       orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Seperate dataframe for when second index is 'sharpe' and 'volatility'\n",
    "results_sharpe = results_df[results_df.index.get_level_values(1) == 'sharpe'].droplevel(1)\n",
    "results_volatility = results_df[results_df.index.get_level_values(1) == 'volatility'].droplevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colours = ['indianred', 'brown', 'maroon', 'slateblue', 'mediumslateblue', 'darkslateblue', 'mediumseagreen', 'gold', 'gray']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(4, 5.2))\n",
    "i=0\n",
    "for date in results_sharpe.index:\n",
    "\n",
    "    # Calculate test period\n",
    "    test_start = datetime.strptime(date, '%Y-%m-%d') + relativedelta(years=3)\n",
    "    test_end = test_start + relativedelta(years=1, months=6)\n",
    "\n",
    "    # Plotting\n",
    "    ax.flatten()[i].bar(results_sharpe[results_sharpe.index==date].columns, np.squeeze(results_sharpe[results_sharpe.index==date].values), color = colours)\n",
    "    plt.setp(ax.flatten()[i].get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "    plt.setp(ax.flatten()[i].get_yticklabels(), fontsize=8)\n",
    "    min_y = np.squeeze(results_sharpe[results_sharpe.index==date].values).min()\n",
    "    max_y = np.squeeze(results_sharpe[results_sharpe.index==date].values).max()\n",
    "    if min_y<0 and max_y<0:\n",
    "        max_y-=0.1\n",
    "        min_y -= 0.1\n",
    "    elif min_y>0 and max_y>0:\n",
    "        min_y=0\n",
    "    ax.flatten()[i].set_ylim(top = max_y+0.1, bottom = min_y)\n",
    "    ax.flatten()[i].set_title(f\"{test_start.strftime('%Y-%m-%d')} - {test_end.strftime('%Y-%m-%d')}\", fontsize = 8)\n",
    "\n",
    "    # Add values to bars\n",
    "    for j, v in enumerate(np.squeeze(results_sharpe[results_sharpe.index==date].values)):\n",
    "        if v > 0:\n",
    "            ax.flatten()[i].text(j, v+0.02, \"{:.2f}\".format(v), color='black', fontsize=6, ha='center')\n",
    "        else:\n",
    "            ax.flatten()[i].text(j, v-0.03, \"{:.2f}\".format(v), color='black', fontsize=6, ha='center')\n",
    "\n",
    "    i+=1\n",
    "\n",
    "ax[0,0].set_ylabel('Sharpe ratio', fontsize=9)\n",
    "ax[1,0].set_ylabel('Sharpe ratio', fontsize=9)\n",
    "\n",
    "fig.suptitle('Maximising the Sharpe Ratio', fontsize=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('plots/max_sharpe_results.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colours = ['indianred', 'brown', 'maroon', 'slateblue', 'mediumslateblue', 'darkslateblue', 'mediumseagreen', 'gold', 'gray']\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize=(4, 5.2))\n",
    "i=0\n",
    "for date in results_volatility.index:\n",
    "\n",
    "    # Calculate test period\n",
    "    test_start = datetime.strptime(date, '%Y-%m-%d') + relativedelta(years=3)\n",
    "    test_end = test_start + relativedelta(years=1, months=6)\n",
    "\n",
    "    # Plotting\n",
    "    ax.flatten()[i].bar(results_volatility[results_volatility.index==date].columns, np.squeeze(results_volatility[results_volatility.index==date].values), color = colours)\n",
    "    plt.setp(ax.flatten()[i].get_xticklabels(), rotation=45, ha='right', fontsize=8)\n",
    "    plt.setp(ax.flatten()[i].get_yticklabels(), fontsize=8)\n",
    "    min_y = np.squeeze(results_volatility[results_volatility.index==date].values).min()\n",
    "    max_y = np.squeeze(results_volatility[results_volatility.index==date].values).max()\n",
    "    if (min_y<0) and (max_y<0):\n",
    "        max_y-=0.1\n",
    "        min_y -= 0.1\n",
    "    elif (min_y>0) and (max_y>0):\n",
    "        min_y=0\n",
    "    else:\n",
    "        min_y-=0.1\n",
    "    ax.flatten()[i].set_ylim(top = max_y+0.1, bottom = min_y)\n",
    "    ax.flatten()[i].set_title(f\"{test_start.strftime('%Y-%m-%d')} - {test_end.strftime('%Y-%m-%d')}\", fontsize = 8)\n",
    "\n",
    "    # Add values to bars\n",
    "    for j, v in enumerate(np.squeeze(results_volatility[results_volatility.index==date].values)):\n",
    "        if v > 0:\n",
    "            ax.flatten()[i].text(j, v+0.02, \"{:.2f}\".format(v), color='black', fontsize=6, ha='center')\n",
    "        else:\n",
    "            ax.flatten()[i].text(j, v-0.03, \"{:.2f}\".format(v), color='black', fontsize=6, ha='center')\n",
    "\n",
    "    i+=1\n",
    "\n",
    "ax[0,0].set_ylabel('Sharpe ratio', fontsize=9)\n",
    "ax[1,0].set_ylabel('Sharpe ratio', fontsize=9)\n",
    "\n",
    "fig.suptitle('Minimising Volatility', fontsize=10)\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('plots/min_vol_results.png');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Average Sharpe ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_sharpe = pd.concat([results_sharpe.mean(axis=0), results_volatility.mean(axis=0)], axis=1)\n",
    "avg_sharpe.columns = ['max_sharpe', 'min_volatility']\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "print(avg_sharpe)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metric_learning_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7701ab843c4e014e6cb61c1bc16af98a2d9e6034c64e4833e09d4584e90230f5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
